{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJ1zvCUxhz_Y"
   },
   "source": [
    "# CS5670 Project 5 - MNIST Challenge Notebook\n",
    "\n",
    "Welcome to the Project 5 4-credit MNIST Challenge.\n",
    "\n",
    "In this notebook you will create and train your own neural network model in PyTorch for classifying handwritten digits on the MNIST dataset.\n",
    "We will pretend that we are targeting low-power and low-memory devices, and so you will have to limit the number of model parameters that you use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4OvSJugE7bql"
   },
   "source": [
    "## Installation of Dependencies\n",
    "\n",
    "First you want to **make sure that you're using the GPU backend**.\n",
    "\n",
    "* Go to Runtime -> Change runtime type and choose \"GPU\" under Hardware Accelerator.\n",
    "* Only then run the next cell. It should print \"GPU Support: True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "x4Wlyq6F7Vny"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Support: True\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision==0.2.1\n",
    "!pip install pillow==6.2.1\n",
    "import torch\n",
    "print(f\"GPU Support: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHLKy2Ekqoq4"
   },
   "source": [
    "## Useful functions\n",
    "\n",
    "Run the below cell to define functions that will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XPWrOExkqysJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import urllib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, sys, math, random, subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from google.protobuf import text_format\n",
    "from io import StringIO\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "def get_n_params(module):\n",
    "    nparam = 0\n",
    "    for name, param in module.named_parameters():\n",
    "        param_count = 1\n",
    "        for size in list(param.size()):\n",
    "            param_count *= size\n",
    "        nparam += param_count\n",
    "    return nparam\n",
    "\n",
    "\n",
    "def get_model_params(model):\n",
    "    nparam = 0\n",
    "    for name, module in model.named_modules():\n",
    "        nparam += get_n_params(module)\n",
    "    return nparam\n",
    "\n",
    "\n",
    "def to_numpy_image(tensor_or_variable):\n",
    "\n",
    "    # If this is already a numpy image, just return it\n",
    "    if type(tensor_or_variable) == np.ndarray:\n",
    "        return tensor_or_variable\n",
    "\n",
    "    # Make sure this is a tensor and not a variable\n",
    "    if type(tensor_or_variable) == Variable:\n",
    "        tensor = tensor_or_variable.data\n",
    "    else:\n",
    "        tensor = tensor_or_variable\n",
    "\n",
    "    # Convert to numpy and move to CPU if necessary\n",
    "    np_img = tensor.cpu().numpy()\n",
    "\n",
    "    # If there is no batch dimension, add one\n",
    "    if len(np_img.shape) == 3:\n",
    "        np_img = np_img[np.newaxis, ...]\n",
    "\n",
    "    # Convert from BxCxHxW (PyTorch convention) to BxHxWxC (OpenCV/numpy convention)\n",
    "    np_img = np_img.transpose(0, 2, 3, 1)\n",
    "\n",
    "    return np_img\n",
    "\n",
    "\n",
    "def normalize_zero_one_range(tensor_like):\n",
    "    x = tensor_like - tensor_like.min()\n",
    "    x = x / (x.max() + 1e-9)\n",
    "    return x\n",
    "\n",
    "\n",
    "def prep_for_showing(image):\n",
    "    np_img = to_numpy_image(image)\n",
    "    if len(np_img.shape) > 3:\n",
    "        np_img = np_img[0]\n",
    "    np_img = normalize_zero_one_range(np_img)\n",
    "    return np_img\n",
    "\n",
    "\n",
    "def show_image(tensor_var_or_np, title=None, bordercolor=None):\n",
    "    np_img = prep_for_showing(tensor_var_or_np)\n",
    "\n",
    "    if bordercolor is not None:\n",
    "        np_img = draw_border(np_img, bordercolor)\n",
    "\n",
    "    # plot it\n",
    "    np_img = np_img.squeeze()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(np_img)\n",
    "    plt.axis('off')\n",
    "    if title: plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJiMTcx9qr5Z"
   },
   "source": [
    "## Training Data\n",
    "\n",
    "We will use the [MNIST handrwritten digit dataset](http://yann.lecun.com/exdb/mnist/) to train our neural network models. There is a simple wrapper for the MNIST dataset in the torchvision package that implements the Dataset class. We will use that in conjunction with the DataLoader to load training data. Run the below cell to download and initialize our training and test datasets. You should see an example batch of images and their labels shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hURbcBfwqUrY"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:  tensor(7) tensor(2) tensor(1) tensor(0) tensor(4) tensor(1) tensor(4) tensor(9) tensor(5) tensor(9) tensor(0) tensor(6) tensor(9) tensor(0) tensor(1) tensor(5) tensor(9) tensor(7) tensor(3) tensor(4) tensor(9) tensor(6) tensor(6) tensor(5) tensor(4) tensor(0) tensor(7) tensor(4) tensor(0) tensor(1) tensor(3) tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data',\n",
    "                                      train=True,\n",
    "                                      download=True,\n",
    "                                      transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data',\n",
    "                                     train=False,\n",
    "                                     download=True,\n",
    "                                     transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=2)\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "show_image(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfN6oCjwrqR1"
   },
   "source": [
    "## Model Definition (Example Model)\n",
    "\n",
    "This is an example model provided to you. It is pretty good and it reaches >98% accuracy on the test set, but we can do better. In particular, we will try to break the 99% accuracy barrier with a tight limit on the number of model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jjvxIDrXrujd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model number of parameters: 20842\n"
     ]
    }
   ],
   "source": [
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExampleModel, self).__init__()\n",
    "        # Convolution. Input channels: 1, output channels: 6, kernel size: 5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Max-pooling layer that will halve the HxW resolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Another 5x5 convolution that brings channel count up to 16\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        # Three fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 60)\n",
    "        self.fc2 = nn.Linear(60, 40)\n",
    "        self.fc3 = nn.Linear(40, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution, activation and pooling\n",
    "        # Output width after convolution = (input_width - (kernel_size - 1) / 2)\n",
    "        # Output width after pooling = input_width / 2\n",
    "\n",
    "        # x.size() = Bx1x28x28\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # x.size() = Bx6x12x12\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # x.size() = Bx16x4x4\n",
    "\n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ExampleModel()\n",
    "\n",
    "nparams = get_n_params(net)\n",
    "print(f\"Model number of parameters: {nparams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujgr4bu-wdD4"
   },
   "source": [
    "## Model Definition (Your Model)\n",
    "\n",
    "Define your own model here. It should satisfy the following constraints:\n",
    "<font color=\"red\">\n",
    "*** Number of total model parameters: < 50000**\n",
    "<br/>\n",
    "*** Reaches test-set accuracy greater than or equal to 99% after training for 10 epochs**\n",
    "</font>\n",
    "  \n",
    "A couple of ideas you can try:\n",
    "* Different model structure (e.g. more layers, smaller/bigger kernels)\n",
    "* Residual connections\n",
    "* Batch [2] / Layer Normalization [3]\n",
    "* Densely connected architectures [1]\n",
    "\n",
    "<font size=\"1em\">[1] Huang, G., Liu, Z., Weinberger, K. Q., & van der Maaten, L. (2017, July). Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (Vol. 1, No. 2, p. 3).</font>\n",
    "</br>\n",
    "<font size=\"1em\">[2] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.</font>\n",
    "</br>\n",
    "<font size=\"1em\">[3] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. arXiv preprint arXiv:1607.06450.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model number of parameters: 47498\n"
     ]
    }
   ],
   "source": [
    "# 9911/10000 in 10 epochs\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        # Define your modules here\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, padding = 1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.conv_dropout = nn.Dropout2d(0.15)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 1 * 1, 128)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(128)\n",
    "        self.fc1_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc_out = nn.Linear(128, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define your dynamic computational graph here\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.adaptivepool(x)\n",
    "        \n",
    "        x = self.conv_dropout(x)\n",
    "\n",
    "        x = x.view(-1, 64 * 1 * 1)\n",
    "        \n",
    "        x = self.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.fc1_dropout(x)\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "net = StudentModel()\n",
    "\n",
    "nparams = get_n_params(net)\n",
    "print(f\"Model number of parameters: {nparams}\")\n",
    "\n",
    "assert nparams < 50000, \"Too many model parameters!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NBY2H8yrgRE"
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "Run the following training loop to train your model. **Uncomment the line `net = StudentModel()` to use your model instead of the example model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EVzqqVy9rfLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "[1,   100] loss: 2.061\n",
      "[1,   200] loss: 1.550\n",
      "[1,   300] loss: 1.164\n",
      "[1,   400] loss: 0.886\n",
      "[1,   500] loss: 0.706\n",
      "[1,   600] loss: 0.566\n",
      "[1,   700] loss: 0.462\n",
      "[1,   800] loss: 0.394\n",
      "[1,   900] loss: 0.369\n",
      "[1,  1000] loss: 0.311\n",
      "[1,  1100] loss: 0.295\n",
      "[1,  1200] loss: 0.267\n",
      "[1,  1300] loss: 0.262\n",
      "[1,  1400] loss: 0.222\n",
      "[1,  1500] loss: 0.221\n",
      "[1,  1600] loss: 0.204\n",
      "[1,  1700] loss: 0.200\n",
      "[1,  1800] loss: 0.186\n",
      "[2,   100] loss: 0.198\n",
      "[2,   200] loss: 0.176\n",
      "[2,   300] loss: 0.156\n",
      "[2,   400] loss: 0.159\n",
      "[2,   500] loss: 0.156\n",
      "[2,   600] loss: 0.155\n",
      "[2,   700] loss: 0.161\n",
      "[2,   800] loss: 0.147\n",
      "[2,   900] loss: 0.156\n",
      "[2,  1000] loss: 0.146\n",
      "[2,  1100] loss: 0.136\n",
      "[2,  1200] loss: 0.124\n",
      "[2,  1300] loss: 0.121\n",
      "[2,  1400] loss: 0.123\n",
      "[2,  1500] loss: 0.126\n",
      "[2,  1600] loss: 0.121\n",
      "[2,  1700] loss: 0.119\n",
      "[2,  1800] loss: 0.099\n",
      "[3,   100] loss: 0.107\n",
      "[3,   200] loss: 0.108\n",
      "[3,   300] loss: 0.108\n",
      "[3,   400] loss: 0.119\n",
      "[3,   500] loss: 0.107\n",
      "[3,   600] loss: 0.105\n",
      "[3,   700] loss: 0.111\n",
      "[3,   800] loss: 0.103\n",
      "[3,   900] loss: 0.092\n",
      "[3,  1000] loss: 0.087\n",
      "[3,  1100] loss: 0.112\n",
      "[3,  1200] loss: 0.096\n",
      "[3,  1300] loss: 0.099\n",
      "[3,  1400] loss: 0.104\n",
      "[3,  1500] loss: 0.107\n",
      "[3,  1600] loss: 0.096\n",
      "[3,  1700] loss: 0.081\n",
      "[3,  1800] loss: 0.100\n",
      "[4,   100] loss: 0.087\n",
      "[4,   200] loss: 0.075\n",
      "[4,   300] loss: 0.075\n",
      "[4,   400] loss: 0.083\n",
      "[4,   500] loss: 0.084\n",
      "[4,   600] loss: 0.084\n",
      "[4,   700] loss: 0.079\n",
      "[4,   800] loss: 0.075\n",
      "[4,   900] loss: 0.093\n",
      "[4,  1000] loss: 0.065\n",
      "[4,  1100] loss: 0.086\n",
      "[4,  1200] loss: 0.081\n",
      "[4,  1300] loss: 0.076\n",
      "[4,  1400] loss: 0.092\n",
      "[4,  1500] loss: 0.078\n",
      "[4,  1600] loss: 0.094\n",
      "[4,  1700] loss: 0.077\n",
      "[4,  1800] loss: 0.072\n",
      "[5,   100] loss: 0.066\n",
      "[5,   200] loss: 0.058\n",
      "[5,   300] loss: 0.072\n",
      "[5,   400] loss: 0.073\n",
      "[5,   500] loss: 0.067\n",
      "[5,   600] loss: 0.070\n",
      "[5,   700] loss: 0.068\n",
      "[5,   800] loss: 0.072\n",
      "[5,   900] loss: 0.071\n",
      "[5,  1000] loss: 0.067\n",
      "[5,  1100] loss: 0.075\n",
      "[5,  1200] loss: 0.060\n",
      "[5,  1300] loss: 0.069\n",
      "[5,  1400] loss: 0.057\n",
      "[5,  1500] loss: 0.062\n",
      "[5,  1600] loss: 0.077\n",
      "[5,  1700] loss: 0.070\n",
      "[5,  1800] loss: 0.061\n",
      "[6,   100] loss: 0.068\n",
      "[6,   200] loss: 0.058\n",
      "[6,   300] loss: 0.061\n",
      "[6,   400] loss: 0.047\n",
      "[6,   500] loss: 0.052\n",
      "[6,   600] loss: 0.057\n",
      "[6,   700] loss: 0.062\n",
      "[6,   800] loss: 0.061\n",
      "[6,   900] loss: 0.056\n",
      "[6,  1000] loss: 0.065\n",
      "[6,  1100] loss: 0.053\n",
      "[6,  1200] loss: 0.061\n",
      "[6,  1300] loss: 0.066\n",
      "[6,  1400] loss: 0.047\n",
      "[6,  1500] loss: 0.051\n",
      "[6,  1600] loss: 0.073\n",
      "[6,  1700] loss: 0.067\n",
      "[6,  1800] loss: 0.057\n",
      "[7,   100] loss: 0.054\n",
      "[7,   200] loss: 0.054\n",
      "[7,   300] loss: 0.050\n",
      "[7,   400] loss: 0.054\n",
      "[7,   500] loss: 0.046\n",
      "[7,   600] loss: 0.057\n",
      "[7,   700] loss: 0.060\n",
      "[7,   800] loss: 0.060\n",
      "[7,   900] loss: 0.048\n",
      "[7,  1000] loss: 0.052\n",
      "[7,  1100] loss: 0.050\n",
      "[7,  1200] loss: 0.050\n",
      "[7,  1300] loss: 0.045\n",
      "[7,  1400] loss: 0.052\n",
      "[7,  1500] loss: 0.056\n",
      "[7,  1600] loss: 0.036\n",
      "[7,  1700] loss: 0.060\n",
      "[7,  1800] loss: 0.048\n",
      "[8,   100] loss: 0.049\n",
      "[8,   200] loss: 0.048\n",
      "[8,   300] loss: 0.064\n",
      "[8,   400] loss: 0.051\n",
      "[8,   500] loss: 0.041\n",
      "[8,   600] loss: 0.047\n",
      "[8,   700] loss: 0.047\n",
      "[8,   800] loss: 0.040\n",
      "[8,   900] loss: 0.049\n",
      "[8,  1000] loss: 0.039\n",
      "[8,  1100] loss: 0.043\n",
      "[8,  1200] loss: 0.044\n",
      "[8,  1300] loss: 0.052\n",
      "[8,  1400] loss: 0.051\n",
      "[8,  1500] loss: 0.058\n",
      "[8,  1600] loss: 0.052\n",
      "[8,  1700] loss: 0.052\n",
      "[8,  1800] loss: 0.046\n",
      "[9,   100] loss: 0.037\n",
      "[9,   200] loss: 0.050\n",
      "[9,   300] loss: 0.039\n",
      "[9,   400] loss: 0.039\n",
      "[9,   500] loss: 0.038\n",
      "[9,   600] loss: 0.043\n",
      "[9,   700] loss: 0.040\n",
      "[9,   800] loss: 0.043\n",
      "[9,   900] loss: 0.040\n",
      "[9,  1000] loss: 0.050\n",
      "[9,  1100] loss: 0.045\n",
      "[9,  1200] loss: 0.049\n",
      "[9,  1300] loss: 0.044\n",
      "[9,  1400] loss: 0.045\n",
      "[9,  1500] loss: 0.037\n",
      "[9,  1600] loss: 0.045\n",
      "[9,  1700] loss: 0.043\n",
      "[9,  1800] loss: 0.049\n",
      "[10,   100] loss: 0.040\n",
      "[10,   200] loss: 0.041\n",
      "[10,   300] loss: 0.040\n",
      "[10,   400] loss: 0.030\n",
      "[10,   500] loss: 0.037\n",
      "[10,   600] loss: 0.036\n",
      "[10,   700] loss: 0.038\n",
      "[10,   800] loss: 0.056\n",
      "[10,   900] loss: 0.041\n",
      "[10,  1000] loss: 0.035\n",
      "[10,  1100] loss: 0.043\n",
      "[10,  1200] loss: 0.042\n",
      "[10,  1300] loss: 0.040\n",
      "[10,  1400] loss: 0.033\n",
      "[10,  1500] loss: 0.038\n",
      "[10,  1600] loss: 0.044\n",
      "[10,  1700] loss: 0.046\n",
      "[10,  1800] loss: 0.044\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "PRINT_EVERY = 100\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "# Comment this line:\n",
    "# net = ExampleModel()\n",
    "\n",
    "# Uncomment this line\n",
    "net = StudentModel()\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "if type(net) == ExampleModel:\n",
    "    print(\n",
    "        \"WARNING! Running example model. Uncomment line to run your own model!\"\n",
    "    )\n",
    "\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "    net = net.cuda()\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Not using GPU\")\n",
    "\n",
    "net.train()\n",
    "\n",
    "# Changing it to 10 epochs as the question asks\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % PRINT_EVERY == PRINT_EVERY - 1:  # print every PRINT_EVERY mini-batches\n",
    "            #show_image(torchvision.utils.make_grid(inputs.data))\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0SLImjx1QE1"
   },
   "source": [
    "## Testing\n",
    "\n",
    "Use the below cell to test your model on the MNIST test set. By right you should only run your model on the test set once, before publishing your results. In this assignment, we will allow you to re-use the test set, which is technically an incorrect practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jo1Zkn5EyNG7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\cv\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99%\n",
      "Correct: 9911/10000\n",
      "\n",
      "Congratulations! You beat the 99% barrier!\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images = Variable(images, volatile=True)\n",
    "    if USE_CUDA:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "acc = 100 * correct / total\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {acc}%')\n",
    "print(f'Correct: {correct}/{total}')\n",
    "print(\"\")\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "if acc >= 99:\n",
    "    print(\"Congratulations! You beat the 99% barrier!\")\n",
    "else:\n",
    "    print(\"Sorry, but you can do better. Try again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KmLl5haX4Om_"
   },
   "source": [
    "### Per-class accuracy\n",
    "\n",
    "Run the below cell to see which digits your model is better at recognizing and which digits it gets confused by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qp8POK0dyOKn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\cv\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of     0 : 99 %\n",
      "Accuracy of     1 : 99 %\n",
      "Accuracy of     2 : 99 %\n",
      "Accuracy of     3 : 99 %\n",
      "Accuracy of     4 : 98 %\n",
      "Accuracy of     5 : 99 %\n",
      "Accuracy of     6 : 98 %\n",
      "Accuracy of     7 : 98 %\n",
      "Accuracy of     8 : 99 %\n",
      "Accuracy of     9 : 98 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "for data in testloader:\n",
    "    images, labels = data\n",
    "    images = Variable(images, volatile=True)\n",
    "    if USE_CUDA:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    c = (predicted == labels).squeeze()\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        class_correct[label] += c[i].cpu().item()\n",
    "        class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' %\n",
    "          (i, 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "CS5670_Project5_MNISTChallenge.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
